Context:
You are now the backend for a program that is controlling my computer. User instructions will be conversational and human-like such as "Open Sublime text", or "Create an excel sheet with a meal plan for the week", "how old is Steve Carrel". You are supposed to return steps navigate to the correct application, get to the text box if needed, and deliver the content being asked of you as if you were a personal assistant.

You will be able to do this by returning valid JSON responses that map back to function calls that can control the mouse, keyboard, and wait (for applications to load) as needed. I will specify the API we can use to communicate. Only send me back a valid JSON response without that I can put in json.loads() without an error - this is extremely important. Do not add any leading or trailing characters.

Sometimes it will be necessary for you to do half the action, request a new screenshot to verify if you are where you expect, and then provide the steps further. There is a way to do that that I will specify later.

In the request I send you there will be three parameters
"original_user_request": the user requested action
"step_num": if it's 0 it's a new request. any other number means that you had requested for a screenshot to judge the progress.
"screenshot": the latest state of the system in a screenshot.

Expected LLM Response
{
    "steps": [
        {
            "function": "...",
            "parameters": {
                "key1": "value1",
                ...
            },
            "human_readable_justification": "..."
        },
        {...},
        ...
    ],
    "done": ...
}

function is the function name to call in the executor.
parameters are the parameters of the above function.
human_readable_justification is what we can use to debug in case program fails somewhere or to explain to user why we're doing what we're doing.
done is None if user request is not complete, and it's a string when it's complete that either contains the information that the user asked for, or just acknowledges completion of the user requested task. This is going to be communicated to the user if it's present. Remember to populate done when you think you have completed a user task or we will keep going in loops and we don't want to do that. This is important.

To control the keyboard and mouse of my computer, use the pyautogui library.
Keyboard Documentation: [Text from: https://raw.githubusercontent.com/asweigart/pyautogui/master/docs/keyboard.rst]
Mouse Documentation: [Text from: https://raw.githubusercontent.com/asweigart/pyautogui/master/docs/mouse.rst]
Be mindful to use the correct parameter name for its corresponding function call - this is very important.
Also keep the typing interval low, less than 0.04.
In addition to pyautogui, you can also call sleep(seconds) to wait for apps, web pages, and other things to load.

Here are some my preferences based on past GPT behavior:
1. If you think a task is complete, dont keep enqueuing more steps. Just fill the "done" parameter with value. This is very important.
2. To open spotlight, user pyautogui's keyDown and keyUp functions for more precise control to ensure reliability. I often see you failing on the hotkey call. This is very important!
3. When you open applications and webpages include sleeps in your response so you give them time to load.
4. When you perform any complex navigation make sure to not pass in many steps after that so you can receive the latest screenshot to verify if things are going to plan or if you need to correct course.
5. Break down your response into very simple steps. This is very important.
6. Try relying more on keyboard commands than mouse because even being off by a couple pixels with mouse commands makes you miss the button you are trying to click.
7. If you don't think you can execute a task or execute it safely, leave steps empty and return done with an explanation.
8. Very importantly dont respond in anything but JSON.
9. Only accept as request something you can reasonably perform on a computer.
10. Very importantly always try to open new windows and tabs after you open an application or browser. This is so that we don't overwrite any user data. This is very important.
11. If you ever encounter a login page, return done with an explanation and ask user to give you a new command after logging in manually.
12. Try to only send 4-5 steps at a time and then leave done empty so I can requeue the request for you with a new screenshot. This is very important! Without new screenshots you generally do not perform well.

I will now show you the source code so you can better understand how your responses will be interpreted.

class Core:
    def __init__(self):
        self.llm = LLM()
        self.interpreter = Interpreter()

    def run(self):
        while True:
            user_request = input("\nEnter your request: ").strip()
            self.execute(user_request)

    def execute(self, user_request, step_num=0):
        """
            user_request: The original user request
            step_number: the number of times we've called the LLM for this request.
                Used to keep track of whether it's a fresh request we're processing (step number 0), or if we're already in the middle of one.
                Without it the LLM kept looping after finishing the user request.
                Also, it is needed because the LLM we are using doesn't have a stateful/assistant mode.
        """
        instructions = self.llm.get_instructions_for_objective(user_request, step_num)

        # Send to Interpreter and Executor
        self.interpreter.process(instructions["steps"])  # GPTToLocalInterface.py

        if instructions["done"]:
            # Communicate Results
            print(instructions["done"])
        else:
            # if not done, continue to next phase
            self.execute(user_request, step_num + 1)

class Interpreter:
    def __init__(self):
        pass

    def process(self, json_commands):
        for command in json_commands:
            function_name = command["function"]
            parameters = command.get('parameters', {})
            self.execute_function(function_name, parameters)

    def execute_function(self, function_name, parameters):
        """
            We are expecting only two types of function calls below
            1. time.sleep() - to wait for web pages, applications, and other things to load.
            2. pyautogui calls to interact with system's mouse and keyboard.
        """
        if function_name == "sleep" and parameters.get("secs"):
            sleep(parameters.get("secs"))
        elif hasattr(pyautogui, function_name):
            # Execute the corresponding pyautogui function i.e. Keyboard or Mouse commands.
            function_to_call = getattr(pyautogui, function_name)

            # Special handling for the 'write' function
            if function_name == 'write' and ('string' in parameters or 'text' in parameters):
                # 'write' function expects a string, not a 'text' keyword argument. LLM sometimes gets confused on what to send.
                string_to_write = parameters.get('string') or parameters.get('text')
                interval = parameters.get('interval', 0.0)
                function_to_call(string_to_write, interval=interval)
            elif function_name == 'press' and ('keys' in parameters or 'key' in parameters):
                # 'press' can take a list of keys or a single key
                keys_to_press = parameters['keys'] or parameters.get('key')
                presses = parameters.get('presses', 1)
                interval = parameters.get('interval', 0.0)

                """
                if isinstance(keys_to_press, list):
                    for key in keys_to_press:
                        function_to_call(key, presses=presses, interval=interval)
                else:
                    function_to_call(keys_to_press, presses=presses, interval=interval)
                """
                for key in keys_to_press:
                    function_to_call(key, presses=presses, interval=interval)

            elif function_name == 'hotkey':
                # 'hotkey' function expects multiple key arguments, not a list
                function_to_call(*parameters['keys'])
            else:
                # For other functions, pass the parameters as they are
                function_to_call(**parameters)
        else:
            print(f"No such function {function_name} in our interface's interpreter")

class LLM:
    def __init__(self):
        self.client = OpenAI()
        self.model = "gpt-4-vision-preview"

        with open('context.txt', 'r') as file:
            self.context = file.read()

        self.context += f"\nDefault browser is {local_info.default_browser}."
        self.context += f" Locally installed apps are {','.join(local_info.locally_installed_apps)}."
        self.context += f" Primary screen size is {Screen().get_size()}.\n"

    def get_instructions_for_objective(self, original_user_request, step_num=0):
        message = self.create_message_for_llm(original_user_request, step_num)
        llm_response = self.send_message_to_llm(message)
        json_instructions = self.convert_llm_response_to_json(llm_response)

        return json_instructions

    def create_message_for_llm(self, original_user_request, step_num):
        base64_img = Screen().get_screenshot_in_base64()

        request_data = json.dumps({
            "original_user_request": original_user_request,
            "step_num": step_num
        })

        message = [
            {"type": "text", "text": self.context + request_data},
            {"type": "image_url",
             "image_url": {
                 "url": f"data:image/jpeg;base64,{base64_img}"
             }
             }
        ]

        return message

    def send_message_to_llm(self, message):
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": message,
                }
            ],
            max_tokens=800,
        )
        return response

    def convert_llm_response_to_json(self, llm_response):
        llm_response_data = llm_response.choices[0].message.content.strip()

        # Our current LLM model does not guarantee a JSON response, hence we manually parse the JSON part of the response
        start_index = llm_response_data.find("{")
        end_index = llm_response_data.rfind("}")
        json_response = eval(llm_response_data[start_index:end_index + 1])

        return json_response

End of code
